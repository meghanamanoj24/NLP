# -*- coding: utf-8 -*-
"""nltk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EUNSVr1fvWjaSKMjo2vBLHRRAeiTigfw
"""

import nltk

import spacy

sentence="natural language processsing is interesting"
sentence

from nltk import word_tokenize

nltk.download('punkt')



# prompt: tokenize the sentence using word tokenize
nltk.download('punkt_tab')

tokens = word_tokenize(sentence)
tokens

# prompt: tokenization using spacy

# If you haven't downloaded the English language model, do so by uncommenting the next line
# !python -m spacy download en_core_web_sm

import spacy

# Load the English language model
nlp = spacy.load("en_core_web_sm")

sentence = "I subscribed to the school newsletter to get weekly updates."

# Process the sentence with SpaCy
doc = nlp(sentence)

# Extract tokens
spacy_tokens = [token.text for token in doc]
spacy_tokens

# prompt: import stemming and do

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

stemmed_tokens = [stemmer.stem(token) for token in spacy_tokens]
stemmed_tokens

# prompt: import lemmatization and do

# Lemmatize tokens
lemmatized_tokens = [token.lemma_ for token in doc]
lemmatized_tokens

# prompt: give  me the code for stop word removal

nltk.download('stopwords')
from nltk.corpus import stopwords

# Stop word removal using NLTK
stop_words = set(stopwords.words('english'))
filtered_tokens_nltk = [token for token in spacy_tokens if token.lower() not in stop_words]
print("Tokens after NLTK stop word removal:", filtered_tokens_nltk)

# Stop word removal using SpaCy
filtered_tokens_spacy = [token.text for token in doc if not token.is_stop]
print("Tokens after SpaCy stop word removal:", filtered_tokens_spacy)